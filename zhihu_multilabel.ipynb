{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhongjc/Installed/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhongjc/Installed/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhongjc/Installed/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhongjc/Installed/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhongjc/Installed/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhongjc/Installed/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertConfig, BertModel, AdamW\n",
    "import torch.utils.data as tud\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch as t\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2020\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "t.manual_seed(SEED)\n",
    "if t.cuda.is_available(): t.cuda.manual_seed(SEED)\n",
    "\n",
    "gpu_ids = [7, 5, 4]\n",
    "MULTI_GPU = False\n",
    "if len(gpu_ids)>1: MULTI_GPU = True\n",
    "device = t.device('cuda:'+str(gpu_ids[0]) if t.cuda.is_available() else 'cpu')\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "MAX_LEN = 256\n",
    "id2tag = {i[1]:i[0] for i in pd.read_csv('./data/topic2id.csv').values}\n",
    "N_TAG = len(id2tag)\n",
    "N_FOLD = 5\n",
    "BATCH_SIZE = 60\n",
    "N_EPOCH = 20\n",
    "df_train = pd.read_csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZHDataset(tud.Dataset):\n",
    "    def __init__(self, datas, max_len):\n",
    "        self.datas = datas\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.datas[idx][1]\n",
    "        if not pd.isnull(data.question_detail): text = data.question_detail\n",
    "        else: text = data.question_title\n",
    "        seq_dict = tokenizer.encode_plus(text, max_length=self.max_len, pad_to_max_length=True, add_special_tokens=True)\n",
    "        input_ids = t.LongTensor(seq_dict['input_ids'])\n",
    "        atten_mask = t.LongTensor(seq_dict['attention_mask'])\n",
    "\n",
    "        label = [0]*N_TAG\n",
    "        if hasattr(data, \"tag_ids\"):\n",
    "            for i in ((str)(data.tag_ids)).split('|'):\n",
    "                i = (int)(i)\n",
    "                label[i-1] = 1\n",
    "        label = t.FloatTensor(label)\n",
    "        return input_ids, atten_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurBert(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(OurBert, self).__init__()\n",
    "        config = BertConfig.from_pretrained('bert-base-chinese', output_hidden_states=True)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese', config=config)\n",
    "        self.fc = nn.Linear(768, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, atten_mask):\n",
    "        output = self.bert(input_ids, atten_mask) # [batch, seqlen, hidden_size]\n",
    "        hidden_states = output[2][-4:]\n",
    "        hidden = torch.stack(hidden_states, dim=-1).max(dim=-1)[0] #[batch, seqlen, hidden_size]        \n",
    "        return self.fc(hidden[:,0,:]) # [batch, n_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_iter, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_cnt, total_loss = 0., 0.\n",
    "    for idx, (input_ids, atten_mask, label) in enumerate(data_iter):\n",
    "        input_ids, atten_mask, label = input_ids.to(device), atten_mask.to(device), label.to(device)\n",
    "        output = model(input_ids, atten_mask)\n",
    "        loss = loss_fn(output, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_cnt += len(input_ids)\n",
    "    return total_loss / total_cnt\n",
    "\n",
    "def evaluate(data_iter, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_cnt, total_loss = 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for idx, (input_ids, atten_mask, label) in enumerate(data_iter):\n",
    "            input_ids, atten_mask, label = input_ids.to(device), atten_mask.to(device), label.to(device)\n",
    "            output = model(input_ids, atten_mask)\n",
    "            loss = loss_fn(output, label)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_cnt += len(input_ids)\n",
    "    model.train()\n",
    "    return total_loss / total_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singleton Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhongjc/Installed/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost: 102 m\n",
      "train loss:175.5247, val loss:22.8703\n",
      "Epoch: 2, Cost: 102 m\n",
      "train loss:19.6697, val loss:17.4917\n",
      "Epoch: 3, Cost: 102 m\n",
      "train loss:16.4822, val loss:16.0587\n",
      "Epoch: 4, Cost: 102 m\n",
      "train loss:15.1026, val loss:15.3922\n",
      "Epoch: 5, Cost: 103 m\n",
      "train loss:14.2153, val loss:15.0587\n",
      "Epoch: 6, Cost: 102 m\n",
      "train loss:13.5262, val loss:14.8989\n",
      "Epoch: 7, Cost: 103 m\n",
      "train loss:12.9516, val loss:14.8116\n",
      "Epoch: 8, Cost: 103 m\n",
      "train loss:12.4478, val loss:14.8028\n",
      "Epoch: 9, Cost: 103 m\n",
      "train loss:11.9969, val loss:14.7421\n",
      "Epoch: 10, Cost: 103 m\n",
      "train loss:11.5745, val loss:14.7913\n",
      "Epoch: 11, Cost: 102 m\n",
      "train loss:11.1844, val loss:14.8812\n",
      "Epoch: 12, Cost: 102 m\n",
      "train loss:10.8096, val loss:14.9377\n",
      "Epoch: 13, Cost: 102 m\n",
      "train loss:10.4552, val loss:15.0077\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d7c531e2159b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#         train_loss, val_loss = 0., 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f19925d0c4f3>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(data_iter, model, loss_fn)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtotal_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matten_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matten_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matten_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matten_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7342c084504d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_detail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_detail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mseq_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0matten_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, pad_to_max_length, is_pretokenized, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m             \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m             \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m         )\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mprepare_for_model\u001b[0;34m(self, ids, pair_ids, max_length, add_special_tokens, stride, truncation_strategy, pad_to_max_length, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_lengths)\u001b[0m\n\u001b[1;32m   2014\u001b[0m             )\n\u001b[1;32m   2015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_for_tokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Installed/anaconda3/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mEncodingFast\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_train_iter = np.array(list(df_train.iterrows()))\n",
    "train_data, val_data = train_test_split(df_train_iter, test_size=0.01, random_state=SEED)\n",
    "train_data = ZHDataset(train_data, MAX_LEN)\n",
    "val_data = ZHDataset(val_data, MAX_LEN)\n",
    "\n",
    "train_iter = tud.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_iter = tud.DataLoader(val_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = OurBert(N_TAG)\n",
    "if MULTI_GPU:\n",
    "    model = nn.DataParallel(model, device_ids=gpu_ids)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "param_optimizer = list(model.named_parameters())  # 模型参数名字列表\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n",
    "val_history = []\n",
    "for e in range(N_EPOCH):\n",
    "    start = time.time()\n",
    "    train_loss = train(train_iter, model,loss_fn, optimizer)\n",
    "    val_loss = evaluate(val_iter,model, loss_fn)\n",
    "\n",
    "#         train_loss, val_loss = 0., 0.\n",
    "    end = time.time()\n",
    "    if len(val_history)==0 or min(val_history)>val_loss:\n",
    "        if MULTI_GPU: t.save(model.module.state_dict(), './models/model_alldetail.pt')\n",
    "        else: t.save(model.state_dict(), './models/model_alldetail.pt')\n",
    "    val_history.append(val_loss)\n",
    "    print(\"Epoch: {}, Cost: {} m\".format(e+1, (int)((end-start)/60)))\n",
    "    print(\"train loss:{:.4f}, val loss:{:.4f}\".format(train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=N_FOLD, random_state=SEED)\n",
    "df_train_iter = np.array(list(df_train.iterrows()))\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train_iter)):\n",
    "    train_data, val_data = df_train_iter[train_idx], df_train_iter[val_idx]\n",
    "    train_data = ZHDataset(train_data, MAX_LEN)\n",
    "    val_data = ZHDataset(val_data, MAX_LEN)\n",
    "    \n",
    "    train_iter = tud.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    val_iter = tud.DataLoader(val_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    model = OurBert(N_TAG)\n",
    "    if MULTI_GPU:\n",
    "        model = nn.DataParallel(model, device_ids=gpu_ids)\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    val_history = []\n",
    "    for e in range(N_EPOCH):\n",
    "        start = time.time()\n",
    "        train_loss = train(train_iter, model,loss_fn, optimizer)\n",
    "        if (e+1)%2==0: val_loss = evaluate(val_iter,model, loss_fn)\n",
    "        \n",
    "#         train_loss, val_loss = 0., 0.\n",
    "        end = time.time()\n",
    "        if len(val_history)==0 or min(val_history)>val_loss:\n",
    "            if MULTI_GPU: t.save(model.module.state_dict(), './models/model_{}.pt'.format(fold))\n",
    "            else: t.save(model.state_dict(), './models/model_{}.pt'.format(fold))\n",
    "        val_history.append(val_loss)\n",
    "        print(\"Fold: {}, Epoch: {}, Cost: {} m\".format(fold, e+1, (int)((end-start)/60)))\n",
    "        print(\"train loss:{:.4f}, val loss:{:.4f}\".format(train_loss, val_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./data/test.csv')\n",
    "test_data = ZHDataset(list(df_test.iterrows()), MAX_LEN)\n",
    "test_iter = tud.DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_labels = []\n",
    "for fold in range(1):\n",
    "    best_model = OurBert(N_TAG).to(device)\n",
    "    best_model.load_state_dict(t.load('./models/model_sigle2.pt'.format(fold), map_location=device))\n",
    "    test_label_fold = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (input_ids, atten_mask, _) in enumerate(test_iter):\n",
    "            input_ids, atten_mask = input_ids.to(device), atten_mask.to(device)\n",
    "            output = best_model(input_ids, atten_mask)  #[batch, n_classes]\n",
    "#             value, arg_idx = torch.topk(output, 5, dim=-1) #[batch, 5]\n",
    "            test_label_fold.extend(output.detach().cpu().numpy())\n",
    "    test_labels.append(test_label_fold)   \n",
    "test_labels = np.array(test_labels)  #[fold, N, n_class]\n",
    "test_labels = test_labels.sum(0) #[N, n_class]\n",
    "test_labels = torch.from_numpy(test_labels)\n",
    "value, arg_idx = torch.topk(test_labels, 5, dim=-1) #[N, 5]\n",
    "test_labels = arg_idx.cpu().numpy()+1\n",
    "submit = pd.DataFrame({'question_id':df_test.question_id, \n",
    "                       '0':test_labels[:,0],\n",
    "                       '1':test_labels[:,1],\n",
    "                       '2':test_labels[:,2],\n",
    "                       '3':test_labels[:,3],\n",
    "                       '4':test_labels[:,4],\n",
    "                      })\n",
    "submit.to_csv(\"./res/submit_test.csv\", index=False, header=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
