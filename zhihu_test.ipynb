{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertConfig, BertModel, AdamW\n",
    "import torch.utils.data as tud\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch as t\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2020\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "t.manual_seed(SEED)\n",
    "if t.cuda.is_available(): t.cuda.manual_seed(SEED)\n",
    "\n",
    "gpu_ids = [2]\n",
    "MULTI_GPU = False\n",
    "if len(gpu_ids)>1: MULTI_GPU = True\n",
    "device = t.device('cuda:'+str(gpu_ids[0]) if t.cuda.is_available() else 'cpu')\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "MAX_LEN = 256\n",
    "id2tag = {i[1]:i[0] for i in pd.read_csv('./data/topic2id.csv').values}\n",
    "N_TAG = len(id2tag)\n",
    "N_FOLD = 5\n",
    "BATCH_SIZE = 350\n",
    "N_EPOCH = 20\n",
    "df_train = pd.read_csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZHDataset(tud.Dataset):\n",
    "    def __init__(self, datas, max_len):\n",
    "        self.datas = datas\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.datas[idx][1]\n",
    "        text = data.question_title\n",
    "        if not pd.isnull(data.question_detail): text += \" \"+data.question_detail\n",
    "        seq_dict = tokenizer.encode_plus(text, max_length=self.max_len, pad_to_max_length=True, add_special_tokens=True)\n",
    "        input_ids = t.LongTensor(seq_dict['input_ids'])\n",
    "        atten_mask = t.LongTensor(seq_dict['attention_mask'])\n",
    "\n",
    "        label = [0]*N_TAG\n",
    "        if hasattr(data, \"tag_ids\"):\n",
    "            for i in ((str)(data.tag_ids)).split('|'):\n",
    "                i = (int)(i)\n",
    "                label[i-1] = 1\n",
    "        label = t.FloatTensor(label)\n",
    "        return input_ids, atten_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurBert(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(OurBert, self).__init__()\n",
    "        config = BertConfig.from_pretrained('bert-base-chinese', output_hidden_states=True)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese', config=config)\n",
    "        self.fc = nn.Linear(768, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, atten_mask):\n",
    "        output = self.bert(input_ids, atten_mask) # [batch, seqlen, hidden_size]\n",
    "        hidden_states = output[2][-4:]\n",
    "        hidden = torch.stack(hidden_states, dim=-1).max(dim=-1)[0] #[batch, seqlen, hidden_size]        \n",
    "        return self.fc(hidden[:,0,:]) # [batch, n_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./data/test.csv')\n",
    "test_data = ZHDataset(list(df_test.iterrows()), MAX_LEN)\n",
    "test_iter = tud.DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_labels = []\n",
    "for fold in range(1):\n",
    "    best_model = OurBert(N_TAG).to(device)\n",
    "    best_model.load_state_dict(t.load('./models/model_sigle1.pt'.format(fold), map_location=device))\n",
    "    test_label_fold = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (input_ids, atten_mask, _) in enumerate(test_iter):\n",
    "            input_ids, atten_mask = input_ids.to(device), atten_mask.to(device)\n",
    "            output = best_model(input_ids, atten_mask)  #[batch, n_classes]\n",
    "            test_label_fold.extend(output.detach().cpu().numpy())\n",
    "    test_labels.append(test_label_fold)   \n",
    "test_labels = np.array(test_labels)  #[fold, N, n_class]\n",
    "test_labels = test_labels.sum(0) #[N, n_class]\n",
    "test_labels = torch.from_numpy(test_labels)\n",
    "value, arg_idx = torch.topk(test_labels, 5, dim=-1) #[N, 5]\n",
    "n_sample = test_labels.shape[0]\n",
    "test_res = []\n",
    "for i in range(n_sample):\n",
    "    tmp = []\n",
    "    for j in range(5):\n",
    "        if t.sigmoid(value[i][j])<0.55 and j>0: tmp.append(-1)\n",
    "        else: tmp.append(arg_idx[i][j].item()+1)\n",
    "    test_res.append(tmp)\n",
    "test_res = np.array(test_res)\n",
    "submit = pd.DataFrame({'question_id':df_test.question_id, \n",
    "                       '0':test_res[:,0],\n",
    "                       '1':test_res[:,1],\n",
    "                       '2':test_res[:,2],\n",
    "                       '3':test_res[:,3],\n",
    "                       '4':test_res[:,4],\n",
    "                      })\n",
    "submit.to_csv(\"./res/submit_test_question0.55.csv\", index=False, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 zhongjc zhongjc 3.3M Jul 18 19:43 ./res/submit_test_pad-1.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./res/submit_test_pad-1.csv\n",
    "\n",
    "\n",
    "test_labels = arg_idx.cpu().numpy()+1\n",
    "pad_res = np.full((test_labels.shape[0], 4), -1)\n",
    "test_labels = np.concatenate((test_labels, pad_res), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())  # 模型参数名字列表\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "NUM_EPOCHS = 3\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=0.05,\n",
    "                     t_total=len(train_loader) * NUM_EPOCHS\n",
    "                    )\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
